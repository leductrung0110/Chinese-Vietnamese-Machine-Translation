# Chinese-Vietnamese Neural Machine Translation for Historical Dramas

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Transformers-orange)](https://huggingface.co/)
[![Colab](https://img.shields.io/badge/Google-Colab-yellow)](https://colab.research.google.com/)

<p align="center">
  <a href="#-tiáº¿ng-viá»‡t">ğŸ‡»ğŸ‡³ Tiáº¿ng Viá»‡t</a> &nbsp;&bull;&nbsp;
  <a href="#-english">ğŸ‡¬ğŸ‡§ English</a>
</p>

---

<a name="-tiáº¿ng-viá»‡t"></a>
## ğŸ‡»ğŸ‡³ Tiáº¿ng Viá»‡t
> **Äá»“ Ã¡n CÃ´ng nghá»‡ ThÃ´ng tin:** Cáº£i thiá»‡n cháº¥t lÆ°á»£ng dá»‹ch mÃ¡y Trung-Viá»‡t dá»±a trÃªn phÃ¢n Ä‘oáº¡n cÃ¢u vÃ  xá»­ lÃ½ cÃ¢u dÃ i trong miá»n dá»¯ liá»‡u phim cá»• trang.

### ğŸ“– Giá»›i thiá»‡u (Introduction)

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng má»™t há»‡ thá»‘ng **Dá»‹ch mÃ¡y Neural (NMT)** chuyÃªn biá»‡t cho cáº·p ngÃ´n ngá»¯ Trung-Viá»‡t, táº­p trung giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c trong phá»¥ Ä‘á» phim cá»• trang:

1.  **Cáº¥u trÃºc cÃ¢u bá»‹ phÃ¢n máº£nh (Fragmented Sentences):** Do giá»›i háº¡n thá»i gian/khÃ´ng gian hiá»ƒn thá»‹ cá»§a phá»¥ Ä‘á».
2.  **RÃ o cáº£n ngÃ´n ngá»¯ cá»• (Archaic Terminology):** Xá»­ lÃ½ cÃ¡c tá»« HÃ¡n-Viá»‡t, xÆ°ng hÃ´ phong kiáº¿n (Tráº«m, Bá»‡ háº¡, Tháº§n thiáº¿p...) vÃ  thÃ nh ngá»¯.

Há»‡ thá»‘ng sá»­ dá»¥ng kiáº¿n trÃºc **Transformer** Ä‘a ngÃ´n ngá»¯ tiÃªn tiáº¿n vá»›i mÃ´ hÃ¬nh ná»n `facebook/nllb-200-distilled-600M` (No Language Left Behind), káº¿t há»£p vá»›i quy trÃ¬nh tiá»n xá»­ lÃ½ dá»¯ liá»‡u thÃ´ng minh (**Context-Aware Pre-processing Pipeline**).

### ğŸš€ TÃ­nh nÄƒng ná»•i báº­t (Key Features)

Dá»± Ã¡n Ä‘á» xuáº¥t hai ká»¹ thuáº­t cá»‘t lÃµi Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u phá»¥ Ä‘á» trÆ°á»›c khi huáº¥n luyá»‡n:

#### 1. PhÃ¢n Ä‘oáº¡n Phá»¥ Ä‘á» ThÃ´ng minh (ISS - Intelligent Subtitle Segmentation)
* Sá»­ dá»¥ng thuáº­t toÃ¡n **Time-based Alignment** (CÄƒn chá»‰nh dá»±a trÃªn thá»i gian).
* Thay vÃ¬ khá»›p dÃ²ng theo chá»‰ sá»‘ (index), thuáº­t toÃ¡n sá»­ dá»¥ng tham sá»‘ `Epsilon = 500ms` Ä‘á»ƒ Ä‘á»“ng bá»™ hÃ³a cÃ¡c Ä‘oáº¡n há»™i thoáº¡i giá»¯a tiáº¿ng Trung vÃ  tiáº¿ng Viá»‡t, Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c vá» máº·t thá»i gian.

#### 2. TÄƒng cÆ°á»ng Ranh giá»›i CÃ¢u (SBA - Sentence Boundary Augmentation)
* Ká»¹ thuáº­t **Probabilistic Merging** (Gá»™p cÃ¢u ngáº«u nhiÃªn) vá»›i xÃ¡c suáº¥t `p=0.3`.
* Tá»± Ä‘á»™ng ná»‘i cÃ¡c Ä‘oáº¡n há»™i thoáº¡i rá»i ráº¡c thÃ nh cÃ¡c cÃ¢u hoÃ n chá»‰nh vá» ngá»¯ nghÄ©a, giÃºp cÆ¡ cháº¿ Attention cá»§a mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c ngá»¯ cáº£nh dÃ i háº¡n (Long-range dependencies).

#### 3. RÃ ng buá»™c Tá»« vá»±ng (Vocabulary Constraint)
* TÃ­ch há»£p tá»« Ä‘iá»ƒn **Chinese-Hanviet Cognates** vÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c thuáº­t ngá»¯ chuyÃªn ngÃ nh vÃ  tá»« HÃ¡n-Viá»‡t Ä‘Æ°á»£c dá»‹ch chÃ­nh xÃ¡c.

### ğŸ“Š Káº¿t quáº£ (Results)

MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ trÃªn táº­p dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao gá»“m **512,580 cáº·p cÃ¢u** Ä‘Æ°á»£c thu tháº­p vÃ  xá»­ lÃ½ tá»« Netflix.

| PhÆ°Æ¡ng phÃ¡p | BLEU Score (Test Set) | Ghi chÃº |
| :--- | :--- | :--- |
| NLLB-200 | **29.35** | *Káº¿t quáº£ tá»‘t nháº¥t* |
| Helsinki-NLP | 11.66 | |
| mBART-50 | 4.25 | *Káº¿t quáº£ dá»‹ch thá»±c táº¿ tá»‘t nháº¥t* |

**So sÃ¡nh Ä‘á»‹nh tÃ­nh:**

* **Input:** çš‡ä¸Šï¼Œè‡£å¦¾çœŸçš„ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠäº†ã€‚
    * *Google Translate:* HoÃ ng thÆ°á»£ng, vá»£ láº½ thá»±c sá»± khÃ´ng biáº¿t pháº£i lÃ m gÃ¬. âŒ
    * *Ours (NLLB-200):* HoÃ ng thÆ°á»£ng, tháº§n thiáº¿p tháº­t sá»± khÃ´ng biáº¿t pháº£i lÃ m sao. âœ…
* **Input:** å‡å¦‚ ä»–æ˜¯åœ¨ç­‰ä»€ä¹ˆäºº (Giáº£ nhÆ° háº¯n Ä‘ang Ä‘á»£i ai Ä‘Ã³)
    * *Google Translate:* Náº¿u anh ta Ä‘ang Ä‘á»£i... (Sai xÆ°ng hÃ´ hiá»‡n Ä‘áº¡i) âŒ
    * *Ours:* Náº¿u háº¯n Ä‘ang Ä‘á»£i... (ÄÃºng sáº¯c thÃ¡i cá»• trang) âœ…

### ğŸ›  CÃ i Ä‘áº·t (Installation)

Dá»± Ã¡n cháº¡y tá»‘t nháº¥t trÃªn **Google Colab** vá»›i GPU. Äá»ƒ cháº¡y cá»¥c bá»™, báº¡n cáº§n cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n sau:

```bash
pip install --upgrade scipy scikit-learn pandas
pip install transformers datasets sacremoses pysrt underthesea \
            sacrebleu unbabel-comet tqdm accelerate evaluate \
            sentencepiece torch "numpy<2.0.0"

```

### ğŸ“‚ Cáº¥u trÃºc dá»± Ã¡n (Project Structure)

```
â”œâ”€â”€ data/                   # Chá»©a dá»¯ liá»‡u thÃ´ (SRT files) tá»« Netflix
â”‚   â”œâ”€â”€ film_A/
â”‚   â”‚   â”œâ”€â”€ zh/             # Phá»¥ Ä‘á» tiáº¿ng Trung
â”‚   â”‚   â””â”€â”€ vi/             # Phá»¥ Ä‘á» tiáº¿ng Viá»‡t
â”œâ”€â”€ workspace_netflix-nllb/
â”‚   â”œâ”€â”€ chinese-hanviet-cognates.tsv  # Tá»« Ä‘iá»ƒn HÃ¡n-Viá»‡t
â”‚   â”œâ”€â”€ final_model/        # ThÆ° má»¥c lÆ°u model NLLB sau khi train
â”‚   â””â”€â”€ eval_results.json   # Káº¿t quáº£ Ä‘Ã¡nh giÃ¡
â”œâ”€â”€ zh_vi_netflix-nllb.ipynb # Source code chÃ­nh (Jupyter Notebook)
â””â”€â”€ README.md

```

### ğŸ’» HÆ°á»›ng dáº«n sá»­ dá»¥ng (Usage)

Quy trÃ¬nh cháº¡y file notebook bao gá»“m cÃ¡c bÆ°á»›c:

1. **Mount Google Drive:** Káº¿t ná»‘i vá»›i nÆ¡i lÆ°u trá»¯ dá»¯ liá»‡u.
2. **Cáº¥u hÃ¬nh:** Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c `data` vÃ  file tá»« Ä‘iá»ƒn.
3. **Tiá»n xá»­ lÃ½ (Preprocessing):** Cháº¡y hÃ m `align_subtitles_by_time` (ISS) vÃ  `sentence_boundary_augmentation` (SBA).
4. **Chuáº©n bá»‹ Dataset:** Code sáº½ tá»± Ä‘á»™ng chia táº­p dá»¯ liá»‡u thÃ nh Train, Validation, Test.
5. **Huáº¥n luyá»‡n (Training):**
* MÃ´ hÃ¬nh: `facebook/nllb-200-distilled-600M`
* Epochs: 3
* Batch size: 4 (káº¿t há»£p Gradient Accumulation = 8)
* Learning rate: 1e-5
* max_length: 128


6. **ÄÃ¡nh giÃ¡ & Demo:** Sá»­ dá»¥ng hÃ m `translate_sentence` Ä‘á»ƒ nháº­p cÃ¢u tÃ¹y Ã½ vÃ  kiá»ƒm tra káº¿t quáº£.

```python
# VÃ­ dá»¥ cháº¡y thá»­
sentence = "å¸ˆå…„ï¼Œæˆ‘ä»¬ä¸€èµ·ä¸‹å±±å§ã€‚"
translate_sentence(sentence)
# Output: SÆ° huynh, chÃºng ta cÃ¹ng xuá»‘ng nÃºi Ä‘i.

```

### ğŸ‘¥ TÃ¡c giáº£ (Authors)

Äá»“ Ã¡n Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi sinh viÃªn Khoa CÃ´ng nghá»‡ ThÃ´ng tin - Äáº¡i há»c TÃ´n Äá»©c Tháº¯ng:

* **LÃª Äá»©c Trung** (MSSV: 522H0110)
* **Phan Thiáº¿t Trung** (MSSV: 522H0071)

**Giáº£ng viÃªn hÆ°á»›ng dáº«n:** TS. Tráº§n Thanh PhÆ°á»›c

### ğŸ“„ License

Dá»± Ã¡n nÃ y phá»¥c vá»¥ má»¥c Ä‘Ã­ch nghiÃªn cá»©u vÃ  há»c táº­p.
Dataset cá»§a dá»± Ã¡n nÃ y Ä‘Æ°á»£c thu tháº­p trÃªn ná»n táº£ng Netflix, náº¿u cÃ³ nhu cáº§u sá»­ dá»¥ng vui lÃ²ng gá»­i tin nháº¯n trá»±c tiáº¿p qua email Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£.

---

<a name="-english"></a>

## ğŸ‡¬ğŸ‡§ English

> **Information Technology Project:** Improving Chinese-Vietnamese machine translation quality based on sentence segmentation and long sentence processing in the historical drama domain.

### ğŸ“– Introduction

This project develops a specialized **Neural Machine Translation (NMT)** system for the Chinese-Vietnamese language pair, specifically addressing challenges in historical drama subtitles:

1. **Fragmented Sentences:** Caused by subtitle display time/space constraints.
2. **Archaic Terminology:** Handling Sino-Vietnamese terms, feudal honorifics (e.g., Your Majesty, Concubine...), and idioms.

The system utilizes the state-of-the-art **NLLB-200 (No Language Left Behind)** architecture (`facebook/nllb-200-distilled-600M`) as the baseline model, integrated with a **Context-Aware Pre-processing Pipeline**.

### ğŸš€ Key Features

We propose two core techniques for processing subtitle data before training:

#### 1. Intelligent Subtitle Segmentation (ISS)

* Utilizes a **Time-based Alignment** algorithm.
* Instead of index-based matching, the algorithm uses an `Epsilon = 500ms` parameter to synchronize Chinese and Vietnamese dialogue segments, ensuring temporal accuracy.

#### 2. Sentence Boundary Augmentation (SBA)

* **Probabilistic Merging** technique with a probability of `p=0.3`.
* Automatically merges fragmented dialogue segments into semantically complete sentences, enabling the model's Attention mechanism to capture long-range dependencies.

#### 3. Vocabulary Constraint

* Integrates a **Chinese-Hanviet Cognates** dictionary into the training process to ensure specialized terms are translated accurately (e.g., translating "é›·éœ†" as "LÃ´i ÄÃ¬nh" instead of the literal "Sáº¥m sÃ©t").

### ğŸ“Š Results

The model was trained and evaluated on a high-quality dataset of **512,580 sentence pairs** collected from Netflix.

| Method | BLEU Score (Test Set) | Note |
| --- | --- | --- |
| NLLB-200 | **29.35** | *Best Performance* |
| Helsinki-NLP | 11.66 |  |
| mBART-50 | 4.25 | *Best actual translation results* |

**Qualitative Comparison:**

* **Input:** çš‡ä¸Šï¼Œè‡£å¦¾çœŸçš„ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠäº†ã€‚ (Your Majesty, I/concubine really don't know what to do.)
* *Google Translate:* ...vá»£ láº½... (Incorrect term "vá»£ láº½") âŒ
* *Ours (NLLB-200):* HoÃ ng thÆ°á»£ng, tháº§n thiáº¿p... (Correct honorific "tháº§n thiáº¿p") âœ…


* **Input:** å‡å¦‚ ä»–æ˜¯åœ¨ç­‰ä»€ä¹ˆäºº (If he is waiting for someone)
* *Google Translate:* Náº¿u anh ta Ä‘ang Ä‘á»£i... (Modern pronoun "anh ta") âŒ
* *Ours:* Náº¿u háº¯n Ä‘ang Ä‘á»£i... (Archaic pronoun "háº¯n") âœ…



### ğŸ›  Installation

The project is best run on **Google Colab** with GPU support. To run locally, install the required libraries:

```bash
pip install --upgrade scipy scikit-learn pandas
pip install transformers datasets sacremoses pysrt underthesea \
            sacrebleu unbabel-comet tqdm accelerate evaluate \
            sentencepiece torch "numpy<2.0.0"

```

### ğŸ’» Usage

Steps to run the `zh_vi_netflix-nllb.ipynb` notebook:

1. **Mount Google Drive:** Connect to your data storage.
2. **Configuration:** Set paths to the `data` directory and dictionary file.
3. **Preprocessing:** Run `align_subtitles_by_time` (ISS) and `sentence_boundary_augmentation` (SBA).
4. **Training:** Fine-tune the model with:
* Model: `facebook/nllb-200-distilled-600M`
* Epochs: 3
* Batch size: 4 (with Gradient Accumulation Steps = 8)
* Learning rate: 1e-5
* max_length: 128


5. **Demo:** Use the `translate_sentence` function to test custom inputs.

```python
sentence = "å¸ˆå…„ï¼Œæˆ‘ä»¬ä¸€èµ·ä¸‹å±±å§ã€‚"
translate_sentence(sentence)
# Output: SÆ° huynh, chÃºng ta cÃ¹ng xuá»‘ng nÃºi Ä‘i.

```

### ğŸ‘¥ Authors

* **Le Duc Trung** (Student ID: 522H0110)
* **Phan Thiet Trung** (Student ID: 522H0071)
* **Supervisor:** Dr. Tran Thanh Phuoc - Ton Duc Thang University

### ğŸ“„ License

This project is for research and educational purposes. The project's dataset was collected from the Netflix platform; if you need to use it, please send a direct message via email for support.

```

```
